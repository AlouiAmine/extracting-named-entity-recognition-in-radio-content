{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Wav2Letter Example using Google Speech Command Dataset\n\nGoogle Speech Command Dataset can be found [here](https://www.kaggle.com/c/tensorflow-speech-recognition-challenge/data). This dataset was chosen as a quick and convenient way to test Wav2Letter performance"},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"Data module. Downloads data. preprocess data. Data feeder pipeline\n\n    TODO:\n        * Build a more memory efficient data feeder pipeline\n\"\"\"\nimport os\nimport numpy as np\nfrom sonopy import mfcc_spec\nfrom scipy.io.wavfile import read\nfrom tqdm import tqdm_notebook as nb_tqdm\nfrom tqdm import tqdm\nimport random\nimport pickle\n\n\nclass IntegerEncode:\n    \"\"\"Encodes labels into integers\n    \n    Args:\n        labels (list): shape (n_samples, strings)\n    \"\"\"\n\n    def __init__(self, labels):\n        # reserve 0 for blank label\n        self.char2index = {\n            \"-\": 0,\n            \"pad\":1\n        }\n        self.index2char = {\n            0: \"-\",\n            1: \"pad\"\n        }\n        self.grapheme_count = 2\n        self.process(labels)\n        self.max_label_seq = 6\n\n    def process(self, labels):\n        \"\"\"builds the encoding values for labels\n        \n        Args:\n            labels (list): shape (n_samples, strings)\n        \"\"\"\n        strings = \"\".join(labels)\n        for s in strings:\n            if s not in self.char2index:\n                self.char2index[s] = self.grapheme_count\n                self.index2char[self.grapheme_count] = s\n                self.grapheme_count += 1\n\n    def convert_to_ints(self, label):\n        \"\"\"Convert into integers\n        \n        Args:\n            label (str): string to encode\n        \n        Returns:\n            list: shape (max_label_seq)\n        \"\"\"\n        y = []\n        for char in label:\n            y.append(self.char2index[char])\n        if len(y) < self.max_label_seq:\n            diff = self.max_label_seq - len(y)\n            pads = [self.char2index[\"pad\"]] * diff\n            y += pads\n        return y\n\n    def save(self, file_path):\n        \"\"\"Save integer encoder model as a pickle file\n\n        Args:\n            file_path (str): path to save pickle object\n        \"\"\"\n        file_name = os.path.join(file_path, \"int_encoder.pkl\")\n        with open(file_name, 'wb') as f:\n            pickle.dump(self.__dict__, f)\n\n\ndef normalize(values):\n    \"\"\"Normalize values to mean 0 and std 1\n    \n    Args:\n        values (np.array): shape (frame_len, features)\n    \n    Returns:\n        np.array: normalized features\n    \"\"\"\n    return (values - np.mean(values)) / np.std(values)\n\n\nclass GoogleSpeechCommand():\n    \"\"\"Data set can be found here \n        https://www.kaggle.com/c/tensorflow-speech-recognition-challenge/data\n    \"\"\"\n\n    def __init__(self, data_path=\"../input/tensorflow-speech-recognition-challenge/train\", sr=16000):\n        self.data_path = data_path\n        self.labels = [\n            'right', 'eight', 'cat', 'tree', 'bed', 'happy', 'go', 'dog', 'no', \n            'wow', 'nine', 'left', 'stop', 'three', 'sheila', 'one', 'bird', 'zero',\n            'seven', 'up', 'marvin', 'two', 'house', 'down', 'six', 'yes', 'on', \n            'five', 'off', 'four'\n        ]\n        self.intencode = IntegerEncode(self.labels)\n        self.sr = sr\n        self.max_frame_len = 225\n\n    def get_data(self, progress_bar=True):\n        \"\"\"Currently returns mfccs and integer encoded data\n\n        Returns:\n            (list, list): \n                inputs shape (sample_size, frame_len, mfcs_features)\n                targets shape (sample_size, seq_len)  seq_len is variable\n        \"\"\"\n        pg = tqdm if progress_bar else lambda x: x\n\n        inputs, targets = [], []\n        meta_data = []\n        for labels in self.labels:\n            \n            path = os.listdir(os.path.join(self.data_path, labels))\n            for audio in path:\n                if i<10:\n                    audio_path = os.path.join(self.data_path, labels, audio)\n                    print(audio_path)\n                    print(labels)\n                    meta_data.append((audio_path, labels))\n                    i+=1\n                else:\n                    break\n                audio_path = os.path.join(self.data_path, labels, audio)\n                meta_data.append((audio_path, labels))\n        \n        random.shuffle(meta_data)\n\n        for md in pg(meta_data):\n            \n            audio_path = md[0]\n            labels = md[1]\n            _, audio = read(audio_path)\n            mfccs = mfcc_spec(\n                audio, self.sr, window_stride=(160, 80),\n                fft_size=512, num_filt=20, num_coeffs=13\n            )\n            mfccs = normalize(mfccs)\n            diff = self.max_frame_len - mfccs.shape[0]\n            mfccs = np.pad(mfccs, ((0, diff), (0, 0)), \"constant\")\n            inputs.append(mfccs)\n\n            target = self.intencode.convert_to_ints(labels)\n            targets.append(target)\n        return inputs, targets\n\n    @staticmethod\n    def save_vectors(file_path, x, y):\n        \"\"\"saves input and targets vectors as x.npy and y.npy\n        \n        Args:\n            file_path (str): path to save numpy array\n            x (list): inputs\n            y (list): targets\n        \"\"\"\n        x_file = os.path.join(file_path, \"x\")\n        y_file = os.path.join(file_path, \"y\")\n        np.save(x_file, np.asarray(x))\n        np.save(y_file, np.asarray(y))\n\n    @staticmethod\n    def load_vectors(file_path):\n        \"\"\"load inputs and targets\n        \n        Args:\n            file_path (str): path to load targets from\n        \n        Returns:\n            inputs, targets: np.array, np.array\n        \"\"\"\n        x_file = os.path.join(file_path, \"x.npy\")\n        y_file = os.path.join(file_path, \"y.npy\")\n\n        inputs = np.load(x_file)\n        targets = np.load(y_file)\n        return inputs, targets\n\n\n# if __name__ == \"__main__\":\n#     gs = GoogleSpeechCommand()\n#     inputs, targets = gs.get_data()\n#     gs.save_vectors(\"../input/\", inputs, targets)\n#     gs.intencode.save(\"../input/\")\n#     print(\"preprocessed and saved\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train\n\"\"\"Trains Wav2Letter model using speech data\n    \n    TODO:\n        * show accuracy metrics\n        * add more diverse datasets\n        * train, val, test split\n\"\"\"\nimport argparse\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n\n\ndef train(batch_size, epochs):\n    # load saved numpy arrays for google speech command\n    gs = GoogleSpeechCommand()\n    _inputs, _targets = gs.load_vectors(\"../input/\")\n\n    # paramters\n    batch_size = batch_size\n    mfcc_features = 13\n    grapheme_count = gs.intencode.grapheme_count\n\n    print(\"training google speech dataset\")\n    print(\"data size\", len(_inputs))\n    print(\"batch_size\", batch_size)\n    print(\"epochs\", epochs)\n    print(\"num_mfcc_features\", mfcc_features)\n    print(\"grapheme_count\", grapheme_count)\n\n    # torch tensors\n    inputs = torch.Tensor(_inputs)\n    targets = torch.IntTensor(_targets)\n\n    print(\"input shape\", inputs.shape)\n    print(\"target shape\", targets.shape)\n\n    # Initialize model, loss, optimizer\n    model = Wav2Letter(mfcc_features, grapheme_count)\n    print(model.layers)\n\n    ctc_loss = nn.CTCLoss()\n    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n\n    # Each mfcc feature is a channel\n    # https://pytorch.org/docs/stable/nn.html#torch.nn.Conv1d\n    # transpose (sample_size, in_frame_len, mfcc_features)\n    # to      (sample_size, mfcc_features, in_frame_len)\n    inputs = inputs.transpose(1, 2)\n    print(\"transposed input\", inputs.shape)\n\n    model.fit(inputs, targets, optimizer, ctc_loss, batch_size, epoch=epochs)\n\n    sample = inputs[0]\n    sample_target = targets[0]\n    \n    log_probs = model.eval(sample)\n    output = GreedyDecoder(log_probs)\n\n    print(\"sample target\", sample_target)\n    print(\"predicted\", output)\n\n\n# if __name__ == \"__main__\":\n#     parser = argparse.ArgumentParser(description='Wav2Letter')\n#     parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n#                         help='input batch size for training (default: 64)')\n#     parser.add_argument('--epochs', type=int, default=100, metavar='N',\n#                         help='total epochs (default: 100)')\n\n#     args = parser.parse_args()\n\n#     batch_size = args.batch_size\n#     epochs = args.epochs\n#     train(batch_size, epochs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"Beam Decoder Module.\n\n    TODO: \n        * write beam search decoder\n        * use KenLM langauge model to aid decoding\n        * add WER and CER metrics\n\"\"\"\nfrom torch import topk\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport numpy as np\nimport pandas as pd\nimport re\nimport copy\n\n#prefix tree\nclass Node:\n    \"class representing nodes in a prefix tree\"\n    def __init__(self):\n        self.children={} # all child elements beginning with current prefix\n        self.isWord=False # does this prefix represent a word\n        \n    def __str__(self):\n        s=''\n        for k in self.children.keys():\n            s+=k\n        return 'isWord: '+str(self.isWord)+'; children: '+s\n\nclass PrefixTree:\n    \"prefix tree\"\n    def __init__(self):\n        self.root=Node()\n\n    def addWord(self, text):\n        \"add word to prefix tree\"\n        node=self.root\n        for i in range(len(text)):\n            c=text[i] # current char\n            if c not in node.children:\n                node.children[c]=Node()\n            node=node.children[c]\n            isLast=(i+1==len(text))\n            if isLast:\n                node.isWord=True\n                \n    def addWords(self, words):\n        for w in words:\n            self.addWord(w)\n                \n                \n    def getNode(self, text):\n        \n        \"get node representing given text\"\n        node=self.root\n        for c in text:\n            if c in node.children:\n                node=node.children[c]\n            else:\n                return None\n        return node\n\n        \n    def isWord(self, text):\n        node=self.getNode(text)\n        if node:\n            return node.isWord\n        return False\n        \n    \n    def getNextChars(self, text):\n        \"get all characters which may directly follow given text\"\n        chars=[]\n        node=self.getNode(text)\n        if node:\n            for k in node.children.keys():\n                chars.append(k)\n        return chars\n    \n    \n    def getNextWords(self, text):\n        \"get all words of which given text is a prefix (including the text itself, it is a word)\"\n        words=[]\n        node=self.getNode(text)\n        if node:\n            nodes=[node]\n            prefixes=[text]\n            while len(nodes)>0:\n                # put all children into list\n                for k,v in nodes[0].children.items():\n                    nodes.append(v)\n                    prefixes.append(prefixes[0]+k)\n                \n                # is current node a word\n                if nodes[0].isWord:\n                    words.append(prefixes[0])\n                \n                # remove current node\n                del nodes[0]\n                del prefixes[0]\n                \n        return words\n                \n                \n    def dump(self):\n        nodes=[self.root]\n        while len(nodes)>0:\n            # put all children into list\n            for v in nodes[0].children.values():\n                nodes.append(v)\n            \n            # dump current node\n            print(nodes[0])\n                \n            # remove from list\n            del nodes[0]\n\n\n\n\nclass Optical:\n    \"optical score of beam\"\n    def __init__(self, prBlank=0, prNonBlank=0):\n        self.prBlank=prBlank # prob of ending with a blank\n        self.prNonBlank=prNonBlank # prob of ending with a non-blank\n\n\nclass Textual:\n    \"textual score of beam\"\n    def __init__(self, text=''):\n        self.text=text\n        self.wordHist=[] # history of words so far\n        self.wordDev='' # developing word\n        self.prUnnormalized=1.0\n        self.prTotal=1.0\n\n\nclass Beam:\n    \"beam with text, optical and textual score\"\n    def __init__(self, lm, useNGrams):\n        \"creates genesis beam\"\n        self.optical=Optical(1.0, 0.0)\n        self.textual=Textual('')\n        self.lm=lm\n        self.useNGrams=useNGrams\n        \n        \n    def mergeBeam(self, beam):\n        \"merge probabilities of two beams with same text\"\n        \n        if self.getText()!=beam.getText():\n            raise Exception('mergeBeam: texts differ')\n        \n        self.optical.prNonBlank+=beam.getPrNonBlank()\n        self.optical.prBlank+=beam.getPrBlank()\n        \n        \n    def getText(self):\n        return self.textual.text\n        \n    \n    def getPrBlank(self):\n        return self.optical.prBlank\n    \n    \n    def getPrNonBlank(self):\n        return self.optical.prNonBlank\n    \n\n    def getPrTotal(self):\n        return self.getPrBlank()+self.getPrNonBlank()\n    \n    \n    def getPrTextual(self):\n        return self.textual.prTotal\n    \n    \n    def getNextChars(self):\n        return self.lm.getNextChars(self.textual.wordDev)\n        \n        \n    def createChildBeam(self, newChar, prBlank, prNonBlank):\n        \"extend beam by new character and set optical score\"\n        beam=Beam(self.lm, self.useNGrams)\n        \n        # copy textual information\n        beam.textual=copy.deepcopy(self.textual)\n        beam.textual.text+=newChar\n        \n        # do textual calculations only if beam gets extended\n        if newChar!='':\n            if self.useNGrams: # use unigrams and bigrams \n            \n                # if new char occurs inside a word\n                if newChar in beam.lm.getWordChars():\n                    beam.textual.wordDev+=newChar\n                    nextWords=beam.lm.getNextWords(beam.textual.wordDev)\n                    \n                    # no complete word in text, then use unigram of all possible next words\n                    numWords=len(beam.textual.wordHist)\n                    prSum=0\n                    if numWords==0:\n                        for w in nextWords:\n                            prSum+=beam.lm.getUnigramProb(w)\n                    # take last complete word and sum up bigrams of all possible next words\n                    else:\n                        lastWord=beam.textual.wordHist[-1]\n                        for w in nextWords:\n                            prSum+=beam.lm.getBigramProb(lastWord, w)\n                    beam.textual.prTotal=beam.textual.prUnnormalized*prSum\n                    beam.textual.prTotal=beam.textual.prTotal**(1/(numWords+1)) if numWords>=1 else beam.textual.prTotal\n                    \n                # if new char does not occur inside a word\n                else:\n                    # if current word is not empty, add it to history\n                    if beam.textual.wordDev!='':\n                        beam.textual.wordHist.append(beam.textual.wordDev)\n                        beam.textual.wordDev=''\n                        \n                        # score with unigram (first word) or bigram (all other words) probability\n                        numWords=len(beam.textual.wordHist)\n                        if numWords==1:\n                            beam.textual.prUnnormalized*=beam.lm.getUnigramProb(beam.textual.wordHist[-1])\n                            beam.textual.prTotal=beam.textual.prUnnormalized\n                        elif numWords>=2:\n                            beam.textual.prUnnormalized*=beam.lm.getBigramProb(beam.textual.wordHist[-2], beam.textual.wordHist[-1])\n                            beam.textual.prTotal=beam.textual.prUnnormalized**(1/numWords)\n            \n            else: # don't use unigrams and bigrams, just keep wordDev up to date\n                if newChar in beam.lm.getWordChars():\n                    beam.textual.wordDev+=newChar\n                else:\n                    beam.textual.wordDev=''\n        \n        # set optical information\n        beam.optical.prBlank=prBlank\n        beam.optical.prNonBlank=prNonBlank\n        return beam\n        \n        \n    def __str__(self):\n        return '\"'+self.getText()+'\"'+';'+str(self.getPrTotal())+';'+str(self.getPrTextual())+';'+str(self.textual.prUnnormalized)\n\n\nclass BeamList:\n    \"list of beams at specific time-step\"\n    def __init__(self):\n        self.beams={}\n        \n\n    def addBeam(self, beam):\n        \"add or merge new beam into list\"\n        # add if text not yet known\n        if beam.getText() not in self.beams:\n            self.beams[beam.getText()]=beam\n        # otherwise merge with existing beam\n        else:\n            self.beams[beam.getText()].mergeBeam(beam)\n        \n        \n    def getBestBeams(self, num):\n        \"return best beams, specify the max. number of beams to be returned (beam width)\"\n        u=[v for (_,v) in self.beams.items()]\n        lmWeight=1\n        return sorted(u, reverse=True, key=lambda x:x.getPrTotal()*(x.getPrTextual()**lmWeight))[:num]\n        \n        \n    def deletePartialBeams(self, lm):\n        \"delete beams for which last word is not finished\"\n        for (k,v) in self.beams.items():\n            lastWord=v.textual.wordDev\n            if (lastWord!='') and (not lm.isWord(lastWord)):\n                del self.beams[k]\n    \n    \n    def completeBeams(self, lm):\n        \"complete beams such that last word is complete word\"\n        for (_,v) in self.beams.items():\n            lastPrefix=v.textual.wordDev\n            if lastPrefix=='' or lm.isWord(lastPrefix):\n                continue\n            \n            # get word candidates for this prefix\n            words=lm.getNextWords(lastPrefix)\n            # if there is just one candidate, then the last prefix can be extended to \n            if len(words)==1:\n                word=words[0]\n                v.textual.text+=word[len(lastPrefix)-len(word):]\n\n\n    def dump(self):\n        for k in self.beams.keys():\n            print(unicode(self.beams[k]).encode('ascii', 'replace')) # map to ascii if possible (for py2 and windows)\n\n\n\nclass LanguageModel:\n    \"unigram/bigram LM, add-k smoothing\"\n    def __init__(self, corpus, chars, wordChars):\n        \"read text from filename, specify chars which are contained in dataset, specify chars which form words\"\n        # read from file\n        self.wordCharPattern='['+wordChars+']'\n        self.wordPattern=self.wordCharPattern+'+'\n        words=re.findall(self.wordPattern, corpus)\n#         print('refindall')\n#         print(corpus)\n#         print(chars)\n#         print(wordChars)\n        uniqueWords=list(set(words)) # make unique\n        self.numWords=len(words)\n        self.numUniqueWords=len(uniqueWords)\n        self.smoothing=True\n        self.addK=1.0 if self.smoothing else 0.0\n        \n        # create unigrams\n        self.unigrams={}\n        for w in words:\n            w=w.lower()\n            if w not in self.unigrams:\n                self.unigrams[w]=0\n            self.unigrams[w]+=1/self.numWords\n        #print('unigrams')\n        #print(self.unigrams)\n        # create unnormalized bigrams\n        bigrams={}\n        for i in range(len(words)-1):\n            w1=words[i].lower()\n            w2=words[i+1].lower()\n            if w1 not in bigrams:\n                bigrams[w1]={}\n            if w2 not in bigrams[w1]:\n                bigrams[w1][w2]=self.addK # add-K\n            bigrams[w1][w2]+=1\n        #print('bigrams')\n        #print(bigrams)\t\n        #normalize bigrams \n        for w1 in bigrams.keys():\n            # sum up\n            probSum=self.numUniqueWords*self.addK # add-K smoothing\n            for w2 in bigrams[w1].keys():\n                probSum+=bigrams[w1][w2]\n            # and divide\n            for w2 in bigrams[w1].keys():\n                bigrams[w1][w2]/=probSum\n        self.bigrams=bigrams\n        #print('normalized bigrams')\n        #print(self.bigrams)\n        # create prefix tree\n        self.tree=PrefixTree() # create empty tree\n        self.tree.addWords(words) # add all unique words to tree\n        \n        # list of all chars, word chars and nonword chars\n        self.allChars=chars\n        self.wordChars=wordChars\n        self.nonWordChars=str().join(set(chars)-set(re.findall(self.wordCharPattern, chars))) # else calculate those chars\n    \n        \n    def getNextWords(self, text):\n        \"text must be prefix of a word\"\n        return self.tree.getNextWords(text)\n        \n        \n    def getNextChars(self, text):\n        \"text must be prefix of a word\"\n        nextChars=str().join(self.tree.getNextChars(text))\n        \n        # if in between two words or if word ends, add non-word chars\n        if (text=='') or (self.isWord(text)):\n            nextChars+=self.getNonWordChars()\n            \n        return nextChars\n\n        \n    def getWordChars(self):\n        return self.wordChars\n\n        \n    def getNonWordChars(self):\n        return self.nonWordChars\n        \n        \n    def getAllChars(self):\n        return self.allChars\n    \n    \n    def isWord(self, text):\n        return self.tree.isWord(text)\n        \n    \n    def getUnigramProb(self, w):\n        \"prob of seeing word w.\"\n        w=w.lower()\n        val=self.unigrams.get(w)\n        if val!=None:\n            return val\n        return 0\n        \n    \n    def getBigramProb(self, w1, w2):\n        \"prob of seeing words w1 w2 next to each other.\"\n        w1=w1.lower()\n        w2=w2.lower()\n        val1=self.bigrams.get(w1)\n        if val1!=None:\n            val2=val1.get(w2)\n            if val2!=None:\n                return val2\n            return self.addK/(self.getUnigramProb(w1)*self.numUniqueWords+self.numUniqueWords)\n        return 0\n\n\ndef wordBeamSearch(mat, beamWidth, lm, useNGrams):\n    \"decode matrix, use given beam width and language model\"\n    chars=lm.getAllChars()\n#     print('all chars')\n#     print(chars)\n    blankIdx=len(chars) # blank label is supposed to be last label in RNN output\n    maxT,_=mat.shape # shape of RNN output: TxC\n    print(mat.shape)\n    genesisBeam=Beam(lm, useNGrams) # empty string\n    last=BeamList() # list of beams at time-step before beginning of RNN output\n    last.addBeam(genesisBeam) # start with genesis beam\n    \n    # go over all time-steps\n    for t in range(maxT):\n        curr=BeamList() # list of beams at current time-step\n    \n        # go over best beams\n        bestBeams=last.getBestBeams(beamWidth) # get best beams\n        for beam in bestBeams:\n            # calc probability that beam ends with non-blank\n            prNonBlank=0\n            if beam.getText()!='':\n                \n                # char at time-step t must also occur at t-1\n                \n                labelIdx=chars.index(beam.getText()[-1])\n                #print(labelIdx)\n                prNonBlank=beam.getPrNonBlank()*mat[t, labelIdx]\n            \n            # calc probability that beam ends with blank\n            prBlank=beam.getPrTotal()*mat[t, blankIdx]\n            \n            # save result\n            curr.addBeam(beam.createChildBeam('', prBlank, prNonBlank))\n            \n            # extend current beam with characters according to language model\n            nextChars=beam.getNextChars()\n            for c in nextChars:\n                # extend current beam with new character\n                labelIdx=chars.index(c)\n                if beam.getText()!='' and beam.getText()[-1]==c: \n                    prNonBlank=mat[t, labelIdx]*beam.getPrBlank() # same chars must be separated by blank\n                else:\n                    prNonBlank=mat[t, labelIdx]*beam.getPrTotal() # different chars can be neighbours\n                    \n                # save result\n                curr.addBeam(beam.createChildBeam(c, 0, prNonBlank))\n        \n        # move current beams to next time-step\n        last=curr\n        \n    # return most probable beam\n    last.completeBeams(lm)\n    bestBeams=last.getBestBeams(1) # sort by probability\n    return bestBeams[0].getText()\n\n#\n# def loadFromCSV(fn):\n#     \"load matrix from csv file. Last entry in row terminated by semicolon.\"\n#     mat=np.genfromtxt(fn, delimiter=';')[:,:-1]\n#     mat=softmax(mat)\n#     return mat\n\ndef softmax(mat):\n    \"calc softmax such that labels per time-step form probability distribution\"\n    # dim0=t, dim1=c\n    maxT,_=mat.shape\n    res=np.zeros(mat.shape)\n    for t in range(maxT):\n        y=mat[t,:]\n        maxValue = np.max(y)\n        e=np.exp(y - maxValue)\n        s=np.sum(e)\n        res[t,:]=e/s\n        \n    return res\ndef GreedyDecoder(ctc_matrix, blank_label=0):\n    \"\"\"Greedy Decoder. Returns highest probability of\n        class labels for each timestep\n\n        # TODO: collapse blank labels\n\n    Args:\n        ctc_matrix (torch.Tensor): \n            shape (1, num_classes, output_len)\n        blank_label (int): blank labels to collapse\n    \n    Returns:\n        torch.Tensor: class labels per time step.\n         shape (ctc timesteps)\n    \"\"\"\n    top = topk(ctc_matrix, k=1, dim=1)\n#     print(top)\n    return top[1][0][0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"s='right\\neight\\ncat\\ntree\\nbed\\nhappy\\ngo\\ndog\\nno\\nwow\\nnine\\nleft\\nstop\\nthree\\nsheila\\none\\nbird\\nzero\\nseven\\nup\\nmarvin\\ntwo\\nhouse\\ndown\\nsix\\nyes\\non\\nfive\\noff\\nfour'\nl=' _rightecabdpyonwlfszvum'\n# print(len(l))\n# print(len(s))\nk=' _rightecabdpyonwlfszvum.'\n# print(s)\ntestLM=LanguageModel(s,l,k)\n\n#testMat=np.array([[0.3, 0.1, 0, 0.6], [0.3, 0.1, 0, 0.6]])\n#testMat=loadFromCSV('/home/aloui/Desktop/eurecom/semesterproject/Wav2Letter-master/CTCWordBeamSearch/data/bentham/mat_0.csv')\t\ntestMat=np.array([[-1.7593e+01, -1.7074e+01, -1.6497e+01, -1.5797e+01, -1.4067e+01,\n      -1.2747e+01, -1.4210e+01, -1.1112e+01, -1.0233e+01, -1.1682e+01,\n      -1.4552e+01, -1.2531e+01, -6.7294e+00, -4.3740e+00, -1.0335e-03,\n      -1.4305e-06],\n     [-1.0432e+02, -1.0189e+02, -9.9412e+01, -9.6165e+01, -8.6178e+01,\n      -7.3011e+01, -6.4005e+01, -5.0625e+01, -4.1224e+01, -3.3498e+01,\n      -2.7419e+01, -1.8274e+01, -1.1610e+01, -1.2879e+01, -1.3572e+01,\n      -1.6149e+01],\n     [-8.5649e+01, -8.3730e+01, -8.1745e+01, -7.8909e+01, -6.8729e+01,\n      -5.5506e+01, -4.6494e+01, -3.2475e+01, -2.3045e+01, -2.0536e+01,\n      -2.3952e+01, -2.6844e+01, -3.0908e+01, -4.0220e+01, -4.3960e+01,\n      -3.9924e+01],\n     [-1.3802e+02, -1.3524e+02, -1.3243e+02, -1.2812e+02, -1.1087e+02,\n      -8.6395e+01, -6.6041e+01, -3.9905e+01, -1.5520e+01, -3.7718e-03,\n      -1.2390e-03, -6.9552e+00, -1.7081e+01, -3.3094e+01, -4.5169e+01,\n      -4.8660e+01],\n     [-9.4878e+01, -9.2692e+01, -9.0491e+01, -8.7345e+01, -7.5866e+01,\n      -6.0044e+01, -4.9246e+01, -3.6678e+01, -2.8755e+01, -2.9582e+01,\n      -3.9844e+01, -4.9655e+01, -5.9207e+01, -7.0509e+01, -7.0579e+01,\n      -5.8878e+01],\n     [-1.2235e+01, -1.1861e+01, -1.1515e+01, -1.0706e+01, -5.7388e+00,\n      -1.1462e-01, -6.2583e-05, -1.0385e-02, -8.1400e+00, -1.6062e+01,\n      -2.2975e+01, -2.8231e+01, -3.6365e+01, -4.7781e+01, -4.9978e+01,\n      -3.6779e+01],\n     [-4.0660e+01, -3.9494e+01, -3.8334e+01, -3.6669e+01, -3.0792e+01,\n      -2.3874e+01, -2.2574e+01, -1.9128e+01, -2.0773e+01, -2.2111e+01,\n      -2.3973e+01, -2.0269e+01, -2.1135e+01, -3.1099e+01, -4.0071e+01,\n      -3.9298e+01],\n     [-5.4299e+01, -5.2935e+01, -5.1557e+01, -4.9247e+01, -3.8925e+01,\n      -2.4997e+01, -1.6878e+01, -4.5741e+00, -3.2789e-04, -5.5843e+00,\n      -1.3035e+01, -1.5696e+01, -2.0302e+01, -3.2494e+01, -4.1780e+01,\n      -3.7952e+01],\n     [-9.9552e+01, -9.7791e+01, -9.6043e+01, -9.3321e+01, -8.2187e+01,\n      -6.7042e+01, -5.7514e+01, -4.5464e+01, -3.9456e+01, -3.7595e+01,\n      -4.2671e+01, -4.7396e+01, -5.1433e+01, -5.9513e+01, -5.8693e+01,\n      -5.0127e+01],\n     [-5.9186e+01, -5.8068e+01, -5.6943e+01, -5.5073e+01, -4.6770e+01,\n      -3.6241e+01, -3.1918e+01, -2.6652e+01, -2.5285e+01, -2.1906e+01,\n      -2.0537e+01, -1.1101e+01, -1.9169e-02, -1.2684e-02, -6.8767e+00,\n      -1.3521e+01],\n     [-9.8360e+01, -9.6137e+01, -9.3839e+01, -9.0783e+01, -8.1057e+01,\n      -6.8911e+01, -6.2822e+01, -5.4668e+01, -5.1669e+01, -5.1261e+01,\n      -5.3135e+01, -5.2876e+01, -5.3515e+01, -5.9671e+01, -6.1255e+01,\n      -5.5138e+01],\n     [-9.6387e+01, -9.4265e+01, -9.2152e+01, -8.9309e+01, -8.0050e+01,\n      -6.8218e+01, -6.1441e+01, -5.3107e+01, -5.0363e+01, -4.7982e+01,\n      -4.6926e+01, -4.4706e+01, -4.6223e+01, -5.4721e+01, -6.2478e+01,\n      -6.3263e+01],\n     [-1.2537e+02, -1.2287e+02, -1.2031e+02, -1.1654e+02, -1.0200e+02,\n      -8.2215e+01, -6.7355e+01, -4.9493e+01, -3.9044e+01, -3.0222e+01,\n      -2.3936e+01, -1.7541e+01, -1.5676e+01, -2.6550e+01, -4.0139e+01,\n      -4.8997e+01],\n     [-1.0041e+02, -9.8510e+01, -9.6565e+01, -9.3949e+01, -8.5550e+01,\n      -7.4019e+01, -6.6600e+01, -5.4862e+01, -4.6085e+01, -3.6859e+01,\n      -3.2397e+01, -2.6417e+01, -2.4829e+01, -3.3564e+01, -4.1811e+01,\n      -4.3082e+01],\n     [-8.7072e+01, -8.5339e+01, -8.3649e+01, -8.1055e+01, -7.0073e+01,\n      -5.5333e+01, -4.5278e+01, -3.2008e+01, -2.3462e+01, -1.8506e+01,\n      -1.7126e+01, -1.7311e+01, -2.3693e+01, -3.7960e+01, -4.7618e+01,\n      -5.0385e+01],\n     [-8.7825e+01, -8.5692e+01, -8.3554e+01, -8.0611e+01, -7.0660e+01,\n      -5.8025e+01, -5.1754e+01, -4.3738e+01, -3.9813e+01, -3.7932e+01,\n      -4.0408e+01, -3.6871e+01, -3.3969e+01, -4.0756e+01, -5.0601e+01,\n      -5.4408e+01],\n     [-9.3954e+01, -9.1868e+01, -8.9801e+01, -8.6735e+01, -7.4910e+01,\n      -5.8734e+01, -4.7331e+01, -3.3974e+01, -2.6063e+01, -2.5609e+01,\n      -3.3082e+01, -4.1594e+01, -5.1939e+01, -6.6132e+01, -7.1978e+01,\n      -6.4930e+01],\n     [-1.0768e+02, -1.0579e+02, -1.0389e+02, -1.0104e+02, -9.0296e+01,\n      -7.5215e+01, -6.4175e+01, -4.9190e+01, -3.4810e+01, -1.7655e+01,\n      -6.6961e+00, -9.7311e-04, -4.0296e+00, -1.9473e+01, -3.2847e+01,\n      -3.7509e+01],\n     [-6.4869e+01, -6.3313e+01, -6.1785e+01, -5.9542e+01, -5.0833e+01,\n      -4.0101e+01, -3.5039e+01, -2.9649e+01, -2.9994e+01, -3.3351e+01,\n      -4.0330e+01, -4.2615e+01, -4.5620e+01, -5.3958e+01, -5.7320e+01,\n      -5.2397e+01],\n     [-4.8876e-06, -7.1525e-06, -1.0014e-05, -2.2530e-05, -3.2248e-03,\n      -2.2230e+00, -9.6915e+00, -1.3734e+01, -1.9076e+01, -2.2994e+01,\n      -2.7448e+01, -3.1231e+01, -3.9155e+01, -5.2370e+01, -5.1645e+01,\n      -3.4997e+01],\n     [-4.8098e+01, -4.6839e+01, -4.5527e+01, -4.3635e+01, -3.6971e+01,\n      -2.8894e+01, -2.7736e+01, -2.6435e+01, -3.2290e+01, -4.1844e+01,\n      -5.2870e+01, -5.9240e+01, -6.2683e+01, -6.7746e+01, -5.9529e+01,\n      -4.6633e+01],\n     [-1.7368e+02, -1.6963e+02, -1.6551e+02, -1.5995e+02, -1.4144e+02,\n      -1.1520e+02, -9.2422e+01, -6.4522e+01, -3.9035e+01, -2.8712e+01,\n      -3.4158e+01, -4.1348e+01, -5.0714e+01, -6.1566e+01, -6.3087e+01,\n      -5.7808e+01],\n     [-1.3419e+02, -1.3163e+02, -1.2914e+02, -1.2557e+02, -1.1194e+02,\n      -9.2902e+01, -7.8236e+01, -5.8599e+01, -4.1839e+01, -3.0906e+01,\n      -3.1228e+01, -3.5177e+01, -4.4230e+01, -5.9774e+01, -6.6112e+01,\n      -6.3609e+01],\n     [-1.1081e+02, -1.0840e+02, -1.0584e+02, -1.0300e+02, -9.7691e+01,\n      -9.2273e+01, -9.1687e+01, -8.7705e+01, -8.7443e+01, -8.8239e+01,\n      -9.0865e+01, -9.0961e+01, -8.9355e+01, -8.9845e+01, -7.9617e+01,\n      -6.5430e+01],\n     [-1.4511e+02, -1.4197e+02, -1.3884e+02, -1.3450e+02, -1.1960e+02,\n      -9.9066e+01, -8.2916e+01, -6.2148e+01, -4.2919e+01, -2.9997e+01,\n      -2.8739e+01, -3.1952e+01, -4.0848e+01, -5.7141e+01, -6.7129e+01,\n      -6.4137e+01]]).T\n# print('shape of matrix')\n# print(testMat.shape)\ntestBW=25\nres=wordBeamSearch(softmax(testMat), testBW, testLM, False)\nprint('Result: \"'+res+'\"')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#model\nfrom __future__ import print_function\nimport math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Wav2Letter(nn.Module):\n    \"\"\"Wav2Letter Speech Recognition model\n        Architecture is based off of Facebooks AI Research paper\n        https://arxiv.org/pdf/1609.03193.pdf\n        This specific architecture accepts mfcc or\n        power spectrums speech signals\n\n        TODO: use cuda if available\n\n        Args:\n            num_features (int): number of mfcc features\n            num_classes (int): number of unique grapheme class labels\n    \"\"\"\n\n    def __init__(self, num_features, num_classes):\n        super(Wav2Letter, self).__init__()\n\n        # Conv1d(in_channels, out_channels, kernel_size, stride)\n        self.layers = nn.Sequential(\n            nn.Conv1d(num_features, 250, 48, 2),\n            torch.nn.ReLU(),\n            nn.Conv1d(250, 250, 7),\n            torch.nn.ReLU(),\n            nn.Conv1d(250, 250, 7),\n            torch.nn.ReLU(),\n            nn.Conv1d(250, 250, 7),\n            torch.nn.ReLU(),\n            nn.Conv1d(250, 250, 7),\n            torch.nn.ReLU(),\n            nn.Conv1d(250, 250, 7),\n            torch.nn.ReLU(),\n            nn.Conv1d(250, 250, 7),\n            torch.nn.ReLU(),\n            nn.Conv1d(250, 250, 7),\n            torch.nn.ReLU(),\n            nn.Conv1d(250, 2000, 32),\n            torch.nn.ReLU(),\n            nn.Conv1d(2000, 2000, 1),\n            torch.nn.ReLU(),\n            nn.Conv1d(2000, num_classes, 1),\n        )\n\n    def forward(self, batch):\n        \"\"\"Forward pass through Wav2Letter network than \n            takes log probability of output\n\n        Args:\n            batch (int): mini batch of data\n             shape (batch, num_features, frame_len)\n\n        Returns:\n            log_probs (torch.Tensor):\n                shape  (batch_size, num_classes, output_len)\n        \"\"\"\n        # y_pred shape (batch_size, num_classes, output_len)\n        y_pred = self.layers(batch)\n\n        # compute log softmax probability on graphemes\n        log_probs = F.log_softmax(y_pred, dim=1)\n\n        return log_probs\n\n    def fit(self, inputs, output, optimizer, ctc_loss, batch_size, epoch, print_every=50):\n        \"\"\"Trains Wav2Letter model.\n\n        Args:\n            inputs (torch.Tensor): shape (sample_size, num_features, frame_len)\n            output (torch.Tensor): shape (sample_size, seq_len)\n            optimizer (nn.optim): pytorch optimizer\n            ctc_loss (ctc_loss_fn): ctc loss function\n            batch_size (int): size of mini batches\n            epoch (int): number of epochs\n            print_every (int): every number of steps to print loss\n        \"\"\"\n\n        total_steps = math.ceil(len(inputs) / batch_size)\n        seq_length = output.shape[1]\n\n        for t in range(epoch):\n\n            samples_processed = 0\n            avg_epoch_loss = 0\n\n            for step in range(total_steps):\n                optimizer.zero_grad()\n                batch = \\\n                    inputs[samples_processed:batch_size + samples_processed]\n\n                # log_probs shape (batch_size, num_classes, output_len)\n                log_probs = self.forward(batch)\n\n                # CTC_Loss expects input shape\n                # (input_length, batch_size, num_classes)\n                log_probs = log_probs.transpose(1, 2).transpose(0, 1)\n\n                # CTC arguments\n                # https://pytorch.org/docs/master/nn.html#torch.nn.CTCLoss\n                # better definitions for ctc arguments\n                # https://discuss.pytorch.org/t/ctcloss-with-warp-ctc-help/8788/3\n                mini_batch_size = len(batch)\n                targets = output[samples_processed: mini_batch_size + samples_processed]\n\n                input_lengths = torch.full((mini_batch_size,), log_probs.shape[0], dtype=torch.long)\n                target_lengths = torch.IntTensor([target.shape[0] for target in targets])\n\n                loss = ctc_loss(log_probs, targets, input_lengths, target_lengths)\n\n                avg_epoch_loss += loss.item()\n\n                loss.backward()\n                optimizer.step()\n\n                samples_processed += mini_batch_size\n\n                if step % print_every == 0:\n                    print(\"epoch\", t + 1, \":\" , \"step\", step + 1, \"/\", total_steps, \", loss \", loss.item())\n\n            print(\"epoch\", t + 1, \"average epoch loss\", avg_epoch_loss / total_steps)\n\n    def eval(self, sample):\n        \"\"\"Evaluate model given a single sample\n\n        Args:\n            sample (torch.Tensor): shape (n_features, frame_len)\n\n        Returns:\n            log probabilities (torch.Tensor):\n                shape (n_features, output_len)\n        \"\"\"\n        _input = sample.reshape(1, sample.shape[0], sample.shape[1])\n        log_prob = self.forward(_input)\n        return log_prob","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# using google's speech command dataset\ngs = GoogleSpeechCommand()\n_inputs, _targets = gs.load_vectors(\"../input/comand-dat/\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# if __name__ == \"__main__\":\n#     gs = GoogleSpeechCommand()\n#     inputs, targets = gs.get_data()\n#     gs.save_vectors(\"./speech_data\", inputs, targets)\n#     gs.intencode.save(\"./speech_data\")\n#     print(\"preprocessed and saved\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mfcc_features = 13\ngrapheme_count = 25\ngrapheme_count","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#del _inputs\n#del _targets","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\ninputs = torch.Tensor(_inputs)\ntargets = torch.IntTensor(_targets)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(inputs.shape)\nprint(targets.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Build Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.nn as nn\nimport torch.optim as optim\n\nmodel = Wav2Letter(mfcc_features, grapheme_count)\nprint(model.layers)\n\nctc_loss = nn.CTCLoss()\noptimizer = optim.Adam(model.parameters(), lr=1e-4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Each mfcc feature is a channel\n# https://pytorch.org/docs/stable/nn.html#torch.nn.Conv1d\n# transpose (sample_size, in_frame_len, mfcc_features)\n# to      (sample_size, mfcc_features, in_frame_len)\ninputs = inputs.transpose(1, 2)\nprint(inputs.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# do short training run\nbatch_size = 1000\nmodel.fit(inputs, targets, optimizer, ctc_loss, batch_size, epoch=10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Evaluate"},{"metadata":{"trusted":true},"cell_type":"code","source":"gs.intencode.index2char","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nsample = inputs[6]\nsample_target = targets[6]\n\nprint(sample.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.save(model.state_dict(),'wav2letter.pt')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_inputs[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#model = TheModelClass(*args, **kwargs)\nmodel.load_state_dict(torch.load('../input/kernelc6ea63747a/wav2letter.pt'))\nlog_prob=model.eval(inputs[3])\noutput = GreedyDecoder(log_prob)\nprint(output)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"log_prob =model.eval(inputs[1])\n\noutput = GreedyDecoder(log_prob)\nprint(output)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mo=''\nfor j in output.numpy() :\n    if j !=1 and j!=0:\n        \n        mo=mo+gs.intencode.index2char[j]\nprint(mo)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mo=''\nfor j in _targets[2] :\n    if j !=1  :\n        \n        mo=mo+gs.intencode.index2char[j]\nprint(mo)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"s='right\\neight\\ncat\\ntree\\nbed\\nhappy\\ngo\\ndog\\nno\\nwow\\nnine\\nleft\\nstop\\nthree\\nsheila\\none\\nbird\\nzero\\nseven\\nup\\nmarvin\\ntwo\\nhouse\\ndown\\nsix\\nyes\\non\\nfive\\noff\\nfour'\nl=' _rightecabdpyonwlfszvum'\n# print(len(l))\n# print(len(s))\nk=' _rightecabdpyonwlfszvum'\n# print(s)\ntestLM=LanguageModel(s,l,k)\ntestMat=log_prob[0].detach().numpy().T\ntestBW=25\nres=wordBeamSearch(softmax(testMat), testBW, testLM, False)\nprint('Result: \"'+res+'\"')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" _, audio = read('../input/bird-audio/0a7c2a8d_nohash_0.wav')\nmfccs = mfcc_spec(\n    audio, 16000, window_stride=(160, 80),\n    fft_size=512, num_filt=20, num_coeffs=13\n)\nmfccs = normalize(mfccs)\ndiff = 225 - mfccs.shape[0]\nmfccs = np.pad(mfccs, ((0, diff), (0, 0)), \"constant\")\nprint(mfccs.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Blank labels are 0, Pads are 1**\n\n**As you can see,  If you remove the 0's and the 1's from the output the model predicted the correct labels!**"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":1}